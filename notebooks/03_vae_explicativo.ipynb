{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé≤ VAE (Variational Autoencoder): Espa√ßo Latente Probabil√≠stico\n",
    "\n",
    "**Tutorial de Espa√ßo Latente - Notebook 3**\n",
    "\n",
    "## üéØ Objetivos\n",
    "- Entender a diferen√ßa entre Autoencoder e VAE\n",
    "- Compreender o truque de reparametriza√ß√£o\n",
    "- Treinar um VAE no MNIST\n",
    "- Gerar novas amostras\n",
    "- Comparar com Autoencoder tradicional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üÜö Autoencoder vs VAE\n",
    "\n",
    "### Autoencoder Tradicional\n",
    "```\n",
    "x ‚Üí Encoder ‚Üí z (ponto fixo) ‚Üí Decoder ‚Üí x'\n",
    "```\n",
    "- z √© determin√≠stico\n",
    "- Espa√ßo latente pode ter \"buracos\"\n",
    "\n",
    "### VAE (Variational Autoencoder)\n",
    "```\n",
    "x ‚Üí Encoder ‚Üí (Œº, œÉ¬≤) ‚Üí z ~ N(Œº, œÉ¬≤) ‚Üí Decoder ‚Üí x'\n",
    "```\n",
    "- z √© uma **distribui√ß√£o** (n√£o um ponto)\n",
    "- Aprende Œº (m√©dia) e œÉ¬≤ (vari√¢ncia)\n",
    "- Espa√ßo latente √© cont√≠nuo e suave\n",
    "- Pode gerar novas amostras!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.models.vae import VAE, vae_loss\n",
    "from src.utils.data_loader import load_mnist\n",
    "from src.utils.training import train_vae\n",
    "from src.utils.visualization import (\n",
    "    plot_vae_results,\n",
    "    visualize_latent_space,\n",
    "    plot_latent_grid,\n",
    "    plot_training_history\n",
    ")\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Carregando Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = load_mnist(batch_size=128)\n",
    "print(f\"Data loaded: {len(train_loader.dataset)} train samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Criando o VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(\n",
    "    input_dim=784,\n",
    "    latent_dim=2,  # 2D para visualiza√ß√£o\n",
    "    hidden_dims=[512, 256]\n",
    ")\n",
    "\n",
    "print(vae)\n",
    "print(f\"\\nParameters: {sum(p.numel() for p in vae.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Fun√ß√£o de Perda do VAE\n",
    "\n",
    "$$\\mathcal{L} = \\underbrace{\\text{BCE}(x, x')}_{\\text{Reconstruction}} + \\beta \\cdot \\underbrace{\\text{KL}(q(z|x) || p(z))}_{\\text{Regulariza√ß√£o}}$$\n",
    "\n",
    "- **Reconstruction Loss**: Qu√£o bem reconstru√≠mos a imagem\n",
    "- **KL Divergence**: For√ßa z a seguir N(0,1)\n",
    "- **Œ≤**: Peso do termo KL (Œ≤=1 para VAE padr√£o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstra√ß√£o da loss\n",
    "data, _ = next(iter(train_loader))\n",
    "data = data.view(-1, 784).to(DEVICE)\n",
    "vae = vae.to(DEVICE)\n",
    "\n",
    "# Forward pass\n",
    "x_recon, mu, logvar, z = vae(data[:4])\n",
    "loss_dict = vae_loss(x_recon, data[:4], mu, logvar, beta=1.0)\n",
    "\n",
    "print(\"Loss components (before training):\")\n",
    "print(f\"  Total: {loss_dict['total'].item():.2f}\")\n",
    "print(f\"  Reconstruction: {loss_dict['reconstruction'].item():.2f}\")\n",
    "print(f\"  KL Divergence: {loss_dict['kl'].item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treina VAE\n",
    "history = train_vae(\n",
    "    model=vae,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=30,\n",
    "    learning_rate=1e-3,\n",
    "    beta=1.0,\n",
    "    device=DEVICE,\n",
    "    early_stopping_patience=7,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Plota hist√≥rico\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Visualiza√ß√£o Completa dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview completo\n",
    "plot_vae_results(vae, test_loader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåå Explorando o Manifold Latente\n",
    "\n",
    "Uma das grandes vantagens do VAE: todo ponto do espa√ßo latente gera uma imagem v√°lida!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid 2D do espa√ßo latente\n",
    "plot_latent_grid(vae, n_samples=20, latent_range=(-3, 3), device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Gerando Novas Amostras\n",
    "\n",
    "VAE pode gerar d√≠gitos totalmente novos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera amostras do prior N(0,1)\n",
    "n_samples = 16\n",
    "samples = vae.sample(num_samples=n_samples, device=DEVICE)\n",
    "samples = samples.view(-1, 28, 28).cpu()\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(samples[i], cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Generated Digits (sampled from N(0,1))', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Compara√ß√£o: VAE vs Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.autoencoder import Autoencoder\n",
    "\n",
    "# Treina autoencoder para compara√ß√£o\n",
    "ae = Autoencoder(input_dim=784, latent_dim=2, hidden_dims=[512, 256, 128])\n",
    "\n",
    "from src.utils.training import train_model\n",
    "history_ae = train_model(\n",
    "    model=ae,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=15,\n",
    "    device=DEVICE,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Visualiza ambos os espa√ßos latentes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Autoencoder\n",
    "ae.eval()\n",
    "latents_ae = []\n",
    "labels_ae = []\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader:\n",
    "        data = data.view(-1, 784).to(DEVICE)\n",
    "        _, latent = ae(data)\n",
    "        latents_ae.append(latent.cpu())\n",
    "        labels_ae.append(labels)\n",
    "latents_ae = torch.cat(latents_ae).numpy()[:1000]\n",
    "labels_ae = torch.cat(labels_ae).numpy()[:1000]\n",
    "\n",
    "ax1.scatter(latents_ae[:, 0], latents_ae[:, 1], c=labels_ae, cmap='tab10', alpha=0.5, s=5)\n",
    "ax1.set_title('Autoencoder Latent Space', fontweight='bold', fontsize=14)\n",
    "ax1.set_xlabel('z[0]')\n",
    "ax1.set_ylabel('z[1]')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# VAE\n",
    "vae.eval()\n",
    "latents_vae = []\n",
    "labels_vae = []\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader:\n",
    "        data = data.view(-1, 784).to(DEVICE)\n",
    "        mu, _ = vae.encode(data)\n",
    "        latents_vae.append(mu.cpu())\n",
    "        labels_vae.append(labels)\n",
    "latents_vae = torch.cat(latents_vae).numpy()[:1000]\n",
    "labels_vae = torch.cat(labels_vae).numpy()[:1000]\n",
    "\n",
    "scatter = ax2.scatter(latents_vae[:, 0], latents_vae[:, 1], c=labels_vae, cmap='tab10', alpha=0.5, s=5)\n",
    "ax2.set_title('VAE Latent Space', fontweight='bold', fontsize=14)\n",
    "ax2.set_xlabel('z[0]')\n",
    "ax2.set_ylabel('z[1]')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.colorbar(scatter, ax=ax2, label='Digit')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Observa√ß√µes:\")\n",
    "print(\"- VAE tem espa√ßo mais 'suave' e cont√≠nuo\")\n",
    "print(\"- Autoencoder pode ter 'buracos' entre clusters\")\n",
    "print(\"- VAE force distribui√ß√£o Gaussiana ‚Üí melhor para gera√ß√£o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Resumo\n",
    "\n",
    "‚úÖ VAE aprende distribui√ß√£o probabil√≠stica no espa√ßo latente  \n",
    "‚úÖ Loss = Reconstruction + KL Divergence  \n",
    "‚úÖ Pode gerar novas amostras facilmente  \n",
    "‚úÖ Espa√ßo latente mais cont√≠nuo que Autoencoder  \n",
    "‚úÖ Trade-off entre qualidade e regulariza√ß√£o (controlado por Œ≤)  \n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Pr√≥ximo Notebook\n",
    "\n",
    "No **Notebook 04**, vamos explorar Beta-VAE e o conceito de disentanglement!\n",
    "\n",
    "‚Üí‚Üí `04_beta_vae_experimento.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
