{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîß Autoencoder B√°sico: Implementa√ß√£o e Treinamento\n",
    "\n",
    "**Tutorial de Espa√ßo Latente - Notebook 2**\n",
    "\n",
    "Neste notebook, vamos implementar e treinar um Autoencoder real no dataset MNIST.\n",
    "\n",
    "## üéØ Objetivos\n",
    "- Entender a arquitetura de um Autoencoder\n",
    "- Treinar o modelo no MNIST\n",
    "- Visualizar o espa√ßo latente 2D\n",
    "- Analisar reconstru√ß√µes\n",
    "- Explorar interpola√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importa nossos m√≥dulos\n",
    "from src.models.autoencoder import Autoencoder\n",
    "from src.utils.data_loader import load_mnist\n",
    "from src.utils.training import train_model\n",
    "from src.utils.visualization import (\n",
    "    visualize_latent_space,\n",
    "    plot_reconstructions,\n",
    "    plot_interpolation,\n",
    "    plot_training_history\n",
    ")\n",
    "\n",
    "# Configura√ß√£o\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Passo 1: Carregando os Dados\n",
    "\n",
    "O MNIST cont√©m 70.000 imagens de d√≠gitos manuscritos (0-9), cada uma com 28x28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega MNIST\n",
    "train_loader, val_loader, test_loader = load_mnist(batch_size=128)\n",
    "\n",
    "print(f\"Train samples: {len(train_loader.dataset)}\")\n",
    "print(f\"Val samples: {len(val_loader.dataset)}\")\n",
    "print(f\"Test samples: {len(test_loader.dataset)}\")\n",
    "\n",
    "# Visualiza algumas imagens\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "fig, axes = plt.subplots(2, 10, figsize=(15, 3))\n",
    "for i in range(20):\n",
    "    ax = axes[i//10, i%10]\n",
    "    ax.imshow(images[i].squeeze(), cmap='gray')\n",
    "    ax.set_title(f'{labels[i].item()}', fontsize=10)\n",
    "    ax.axis('off')\n",
    "plt.suptitle('MNIST Dataset Samples', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Passo 2: Criando o Autoencoder\n",
    "\n",
    "Nossa arquitetura:\n",
    "```\n",
    "Input (784) ‚Üí [512] ‚Üí [256] ‚Üí [128] ‚Üí Latent (2) ‚Üí [128] ‚Üí [256] ‚Üí [512] ‚Üí Output (784)\n",
    "            Encoder                                           Decoder\n",
    "```\n",
    "\n",
    "Taxa de compress√£o: 784 / 2 = **392x**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria modelo\n",
    "model = Autoencoder(\n",
    "    input_dim=784,\n",
    "    latent_dim=2,  # 2D para visualiza√ß√£o\n",
    "    hidden_dims=[512, 256, 128]\n",
    ")\n",
    "\n",
    "print(model)\n",
    "print(f\"\\nCompression ratio: {model.compression_ratio():.1f}x\")\n",
    "\n",
    "# Conta par√¢metros\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Passo 3: Treinamento\n",
    "\n",
    "Vamos treinar o modelo para minimizar o erro de reconstru√ß√£o (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treina\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=20,\n",
    "    learning_rate=1e-3,\n",
    "    device=DEVICE,\n",
    "    early_stopping_patience=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Plota curvas de treinamento\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Passo 4: Visualizando o Espa√ßo Latente\n",
    "\n",
    "Vamos ver como o modelo organizou os d√≠gitos no espa√ßo 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza espa√ßo latente\n",
    "visualize_latent_space(model, test_loader, device=DEVICE,\n",
    "                      title='Autoencoder Latent Space (MNIST)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An√°lise:**\n",
    "- D√≠gitos similares ficam pr√≥ximos no espa√ßo latente\n",
    "- Clusters claramente separados\n",
    "- Apenas 2 dimens√µes capturam a ess√™ncia de 784 pixels!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Passo 5: Analisando Reconstru√ß√µes\n",
    "\n",
    "Qu√£o bem o modelo reconstr√≥i as imagens originais?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plota reconstru√ß√µes\n",
    "plot_reconstructions(model, test_loader, n_samples=10, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåä Passo 6: Interpola√ß√£o no Espa√ßo Latente\n",
    "\n",
    "O que acontece quando \"caminhamos\" entre dois d√≠gitos no espa√ßo latente?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pega dois d√≠gitos\n",
    "data, labels = next(iter(test_loader))\n",
    "\n",
    "# Interpola entre primeiro e segundo d√≠gito\n",
    "plot_interpolation(model, data[0], data[5], num_steps=10, device=DEVICE)\n",
    "\n",
    "print(f\"\\nInterpolating from digit {labels[0]} to digit {labels[5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Passo 7: Explorando Manualmente o Espa√ßo Latente\n",
    "\n",
    "Vamos decodificar pontos arbitr√°rios do espa√ßo latente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Define pontos no espa√ßo latente\n",
    "latent_points = torch.tensor([\n",
    "    [0, 0],    # Centro\n",
    "    [-2, -2],  # Canto inferior esquerdo\n",
    "    [2, 2],    # Canto superior direito\n",
    "    [-2, 2],   # Canto superior esquerdo\n",
    "    [2, -2],   # Canto inferior direito\n",
    "    [0, 3],    # Topo\n",
    "    [3, 0],    # Direita\n",
    "], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "# Decodifica\n",
    "with torch.no_grad():\n",
    "    decoded = model.decode(latent_points)\n",
    "    decoded = decoded.view(-1, 28, 28).cpu()\n",
    "\n",
    "# Visualiza\n",
    "fig, axes = plt.subplots(1, 7, figsize=(14, 2))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(decoded[i], cmap='gray')\n",
    "    ax.set_title(f'z={latent_points[i].cpu().numpy()}', fontsize=8)\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Decoded Images from Latent Points', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Passo 8: An√°lise Quantitativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.training import evaluate_model\n",
    "\n",
    "# Avalia no test set\n",
    "test_metrics = evaluate_model(model, test_loader, device=DEVICE, is_vae=False)\n",
    "\n",
    "print(\"Test Set Metrics:\")\n",
    "print(f\"  Loss (MSE): {test_metrics['loss']:.6f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  - Average pixel error: {np.sqrt(test_metrics['loss']):.4f}\")\n",
    "print(f\"  - Compression: 784 ‚Üí 2 dimensions ({model.compression_ratio():.0f}x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Experimento: Diferentes Dimens√µes Latentes\n",
    "\n",
    "O que acontece com dimens√µes latentes maiores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testa diferentes latent_dims\n",
    "latent_dims = [2, 5, 10, 20, 50]\n",
    "results = []\n",
    "\n",
    "for latent_dim in latent_dims:\n",
    "    print(f\"\\nTraining with latent_dim={latent_dim}...\")\n",
    "    \n",
    "    model_temp = Autoencoder(input_dim=784, latent_dim=latent_dim)\n",
    "    \n",
    "    history_temp = train_model(\n",
    "        model=model_temp,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_epochs=10,\n",
    "        device=DEVICE,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    final_loss = history_temp['val_loss'][-1]\n",
    "    results.append((latent_dim, final_loss))\n",
    "    print(f\"  Final val loss: {final_loss:.6f}\")\n",
    "\n",
    "# Plota\n",
    "dims, losses = zip(*results)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dims, losses, marker='o', linewidth=2, markersize=10)\n",
    "plt.xlabel('Latent Dimension', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Validation Loss', fontsize=12, fontweight='bold')\n",
    "plt.title('Reconstruction Quality vs Latent Dimension', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Observa√ß√£o:\")\n",
    "print(\"- Dimens√µes maiores ‚Üí melhor reconstru√ß√£o\")\n",
    "print(\"- Mas perde interpretabilidade e compress√£o\")\n",
    "print(\"- Trade-off entre compress√£o e qualidade\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Resumo\n",
    "\n",
    "Neste notebook, aprendemos:\n",
    "\n",
    "‚úÖ Como treinar um Autoencoder em PyTorch  \n",
    "‚úÖ Visualizar o espa√ßo latente aprendido  \n",
    "‚úÖ Analisar reconstru√ß√µes  \n",
    "‚úÖ Interpolar no espa√ßo latente  \n",
    "‚úÖ Trade-off entre compress√£o e qualidade  \n",
    "\n",
    "**Limita√ß√µes do Autoencoder:**\n",
    "- Espa√ßo latente n√£o √© cont√≠nuo (buracos)\n",
    "- N√£o pode gerar novas amostras facilmente\n",
    "- Aprende mapeamento determin√≠stico\n",
    "\n",
    "**Solu√ß√£o:** Variational Autoencoder (VAE)!\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Pr√≥ximo Notebook\n",
    "\n",
    "No **Notebook 03**, vamos explorar VAEs que resolvem essas limita√ß√µes!\n",
    "\n",
    "‚Üí‚Üí `03_vae_explicativo.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
